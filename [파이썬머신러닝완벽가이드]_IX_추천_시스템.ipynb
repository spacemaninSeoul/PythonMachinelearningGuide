{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "[파이썬머신러닝완벽가이드] IX. 추천 시스템.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "mount_file_id": "1UE6kjYI5-Ybld-ZzelGc7LFF8w4BcwOx",
      "authorship_tag": "ABX9TyM0QSczE1XbgeVvMdIcRp42",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/spacemaninSeoul/PythonMachinelearningGuide/blob/main/%5B%ED%8C%8C%EC%9D%B4%EC%8D%AC%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D%EC%99%84%EB%B2%BD%EA%B0%80%EC%9D%B4%EB%93%9C%5D_IX_%EC%B6%94%EC%B2%9C_%EC%8B%9C%EC%8A%A4%ED%85%9C.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7c7RBKVqkF7H"
      },
      "source": [
        "# 1. 추천 시스템의 개요와 배경"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5-tzHptpkTKo"
      },
      "source": [
        "## 1) 추천 시스템의 개요와 배경\n",
        "\n",
        "추천 시스템의 진정한 묘미는 사용자 자신도 좋아하는지 몰랐던 취향을 시스템이 발견하고 그에 맞는 콘텐츠를 추천해주는 것이다. 이러한 추천 시스템을 접한 사용자는 해당 사이트를 더 강하게 신뢰하게 되어 더 많은 추천 콘텐츠를 선택하게 된다. 결국 더 많은 데이터가 추천 시스템에 축적되면서 추천이 더욱 정확해지고 다양한 결과를 얻을 수 있는 좋은 선순환 시스템을 구축할 수 있게 된다.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KO-MaCkEknxL"
      },
      "source": [
        "## 2) 추천 시스템의 유형\n",
        "\n",
        "추천 시스템은 크게 **콘텐츠 기반 필터링(Content based filtering) 방식**과 **협업 필터링(Collaborative Filtering)** 방식으로 나뉜다. 그리고 협업 필터림 방식은 다시 **최근접 이웃(Nearest Neighbor) 협업 필터링**과 **잠재 요인(Latent Factor) 협업 필터링**으로 나뉜다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t-iC-R3ClF3b"
      },
      "source": [
        "# 2. 콘텐츠 기반 필터링 추천 시스템\n",
        "\n",
        "**콘텐츠 기반 필터링 방식**은 사용자가 특정한 아이템을 매우 선호하는 경우, 그 아이템과 비슷한 콘텐츠를 가진 다른 아이템을 추천하는 방식이다.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fBE5xDwilU2S"
      },
      "source": [
        "# 3. 최근접 이웃 협업 필터링\n",
        "\n",
        "친구들에게 물어보는 것과 유사한 방식으로, 사용자가 아이템에 매긴 평점 정보나 상품 구매 이력과 같은 사용자 행동 양식(User Behavior)만을 기반으로 추천을 수행하는 것이 협업 필터링(Collaborative Filtering) 방식이다.\n",
        "\n",
        "협업 필터링의 주요 목표는 사용자-아이템 평점 매트릭스와 같은 축적된 사용자 행동 데이터를 기반으로 사용자가 아직 평가하지 않은 아이템을 예측 평가(Predicted Rating)하는 것이다.\n",
        "\n",
        "협업 필터링 기반의 추천 시스템은 최근접 이웃 방식과 잠재 요인 방식으로 나뉘며, 두 방식 모두 사용자-아이템 평점 행렬 데이터에만 의지해 추천을 수행한다. 협업 필터링 알고리즘에 사용되는 사용자-아이템 평점 행렬에서 행(Row)은 개별 사용자, 열(Column)은 개별 아이템으로 구성되며, 사용자 아이디 행, 아이템 아이디 열 위치에 해당하는 값이 평점을 나타내는 형태가 돼야 한다.\n",
        "\n",
        "일반적으로 이러한 사용자-아이템 평점 행렬은 많은 아이템을 열로 가지는 다차원 행렬이며, 사용자가 아이템에 대한 평점을 매기는 경우가 많지 않기 때문에 희소 행렬(Sparse Matrix) 특성을 가진다.\n",
        "\n",
        "최근접 이웃 협업 필터링은 메모리(Memory) 협업 필터링이라고도 하며, 일반적으로 사용자 기반과 아이템 기반으로 다시 나뉠 수 있다.\n",
        "- 사용자 기반(User-User): 당신과 비슷한 고객들이 다음 상품도 구매했습니다.\n",
        "- 아이템 기반(Item-Item): 이 상품을 선택한 다른 고객들은 다음 상품도 구매했습니다.\n",
        "\n",
        "사용자 기반 최근접 이웃 방식은 특정 사용자와 유사한 다른 사용자를 TOP-N으로 선정해 이 TOP-N 사용자가 좋아하는 아이템을 추천하는 방식이다. 즉, 특정 사용자와 타 사용자 간의 유사도(Similarity)를 측정한 뒤 가장 유사도가 높은 TOP-N 사용자를 추출해 그들이 선호하는 아이템을 추천하는 것이다.\n",
        "\n",
        "아이템 기반 최근접 이웃 방식은 그 명칭이 주는 이미지 때문에 '아이템 간 속성'이 얼마나 비슷한지를 기반으로 추천한다고 착각할 수 있다. 하지만 아이템 기반 최근접 이웃 방식은 아이템이 가지는 속성과는 상관없이 사용자들이 그 아이템을 좋아하는지/싫어하는지 평가 척도가 유사한 아이템을 추천하는 기준이 되는 알고리즘이다.\n",
        "\n",
        "일반적으로 사용자 기반보다는 아이템 기반 협업 필터링이 정확도가 더 높다. 이유는 비슷한 영화(또는 상품)를 좋아(또는 구입)한다고 해서 사람들의 취향이 비슷하다고 판단하기는 어려운 경우가 많기 때문이다. 매우 유명한 영화는 취향과 관계없이 대부분의 사람이 관람하는 경우가 많고, 사용자들이 평점을 매긴 영화(또는 상품)의 개수가 많지 않은 경우가 일반적인데 이를 기반으로 다른 사람과의 유사도를 비교하기가 어려운 부분도 있다. 따라서 최근접 이웃 협업 필터링은 대부분 아이템 기반의 알고리즘을 적용한다.\n",
        "\n",
        "앞장의 텍스트 분석에서 소개된 유사도 측정 방법인 코사인 유사도는 추천 시스템의 유사도 측정에 가장 많이 적용된다.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "djlA4vXVqZ2R"
      },
      "source": [
        "# 4. 잠재 요인 협업 필터링\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "26v7dqm6qh7i"
      },
      "source": [
        "## 1) 잠재 요인 협업 필터링의 이해\n",
        "\n",
        "**잠재 요인 협업 필터링**은 사용자-아이템 평점 매트릭스 속에 숨어 있는 잠재 요인을 추출해 추천 예측을 할 수 있게 하는 기법이다. 대규모 다차원 행렬을 SVD와 같은 차원 감소 기법으로 분해하는 과정에서 잠재 요인을 추출하는데, 이러한 기법을 행렬 분해(Matrix Factorization)이라고 한다.\n",
        "\n",
        "잠재 요인 협업 필터링은 사용자-아이템 평점 행렬 데이터만을 이용해 말 그대로 '잠재 요인'을 끄집어 내는 것을 의미한다. '잠재 요인'이 어떤 것인지는 명확히 정의할 수 없다. 하지만 이러한 '잠재 요인'을 기반으로 다차원 희소 행렬인 사용자-아이템 행렬 데이터를 저차원 밀집 행렬의 사용자-잠재 요인 행렬과 아이템-잠재 요인 행렬의 전치 행렬(즉, 잠재 요인-아이템 행렬)로 분해할 수 있으며, 이렇게 분해된 두 행렬의 내적을 통해 새로운 예측 사용자-아이템 평점 행렬 데이터를 만들어서 사용자가 아직 평점을 부여하지 않는 아이템에 대한 예측 평점을 생성하는 것이 잠재 요인 협력 필터링 알고리즘의 골자이다.\n",
        "\n",
        "행렬 분해에 의해 추출되는 '잠재 요인'이 정확히 어떤 것인지는 알 수 없지만, 가령 영화 평점 기반의 사용자-아이템 평점 행렬 데이터라면 영화가 가지는 장르별 특성 선호도로 가정할 수 있다. 즉, 사용자-잠재 요인 행렬은 영화 장르에 대한 선호도로, 아이템-잠재 요인 행렬은 영화의 장르별 특성값으로 정의할 수 있다.\n",
        "\n",
        "잠재 요인 협업 필터링은 숨겨져 있는 '잠재 요인'을 기반으로 분해된 매트릭스를 이용해 사용자가 아직 평가하지 않은 아이템에 대한 예측 평가를 수행하는 것이다. 사용자-아이템 평점 행렬과 같이 다차원의 매트릭스를 저차원의 매트릭스로 분해하는 기법을 **행렬 분해(Matrix Factorization)**이라고 한다.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TSkQy6oZIFbs"
      },
      "source": [
        "## 2) 행렬 분해의 이해\n",
        "\n",
        "**행렬 분해**는 다차원의 매트릭스를 저차원 매트릭스로 분해하는 기법으로서 대표적으로 SVD(Singular Vector Decomposition), NMF(Non-Negative Matrix Factorization) 등이 있다. Factorization(분해)은 우리말로 '인수분해'를 말한다.\n",
        "\n",
        "M개의 사용자(User) 행과 N개의 아이템(Item) 열을 가진 평점 행렬 R은 M X N 차원으로 구성되며, 행렬 분해를 통해서 사용자-K 차원 잠재 요인 행렬 P(P는 M X K 차원)와 K 차원 잠재 요인 - 아이템 행렬 Q.T(Q.T는 K X N 차원)로 분해될 수 있다(Q는 아이템-잠재 요인 행렬이며, Q.T는 Q의 전치 행렬인 잠재 요인-아이템 행렬이다).\n",
        "\n",
        "즉, R = P * Q.T 이며 각 기호에 대한 설명은 다음과 같다.\n",
        " - M은 총 사용자 수\n",
        " - N은 총 아이템 수\n",
        " - K는 잠재 요인의 차원 수\n",
        " - R은 M X N 차원의 사용자-아이템 평점 행렬\n",
        " - P는 사용자와 잠재 요인과의 관계 값을 가지는 M X K 차원의 사용자-잠재 요인 행렬\n",
        " - Q는 아이템과 잠재 요인과의 관계 값을 가지는 N X K 차원의 아이템-잠재 요인 행렬\n",
        " - Q.T는 Q 매트릭스의 행과 열 값을 교환한 전치 행렬\n",
        "\n",
        "\n",
        " R 행렬의 u행 사용자와 i열 아이템 위치에 있는 평점 데이터를 r(u, i)라고 하면\n",
        " > r(u, i) = pu * qi^t\n",
        "로 유추할 수 있다. 여기서 pu는 P 행렬에서 u행 사용자 벡터이며, qi^t는 Q 행렬의 i행 아이템 벡터의 전치 벡터이다.\n",
        "\n",
        "사용자-아이템 평점 행렬의 미정 값을 포함한 모든 평점 값은 행렬 분해를 통해 얻어진 P 행렬과 Q.T 행렬의 내적을 통해 예측 평점으로 다시 계산할 수 있다.\n",
        "> R ~= ^R = P * Q.T\n",
        "\n",
        "그렇다면 R 행렬을 어떻게 P와 Q 행렬로 분해할까? 행렬 분해는 주로 SVD(Singular Value Decomposition) 방식을 이용한다. 하지만 SVD는 널(NaN) 값이 없는 행렬에만 적용할 수 있다. R 행렬에는 아직 평점이 되지 않은 많은 널 값이 있기 때문에 P와 Q 행렬을 일반적인 SVD 방식으로는 분해할 수 없다. 이러한 경우에는 확률적 경사 하강법(Stochastic Gradient Descent, SGD)이나 ALS(Alternating Least Squares) 방식을 이용해 SVD를 수행한다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J8u_qz85OErI"
      },
      "source": [
        "## 3) 확률적 경사 하강법을 이용한 행렬 분해\n",
        "\n",
        "확률적 경사 하강법을 이용한 행렬 분해 방법을 요약하자면, P와 Q 행렬로 계산된 예측 R 행렬 값이 실제 R 행렬 값과 가장 최소의 오류를 가질 수 있도록 반복적인 비용 함수 최적화를 통해 P와 Q를 유추해내는 것이다.\n",
        "\n",
        "확률적 경사 하강법(이하 SGD)을 이용한 행렬 분해의 전반적인 절차를 보면 조금 더 쉽게 이해할 수 있다. \n",
        "  1. P와 Q를 임의의 값을 가진 행렬로 설정한다.\n",
        "  2. P와 Q.T 값을 곱해 예측 R 행렬을 계산하고 예측 R 행렬과 실제 R 행렬에 해당하는 오류 값을 계산한다.\n",
        "  3. 이 오류 값을 최소화할 수 있도록 P와 Q 행렬을 적절한 값으로 각각 업데이트한다.\n",
        "  4. 만족할 만한 오류 값을 가질 때까지 2,3 번 작업을 반복하면서 P와 Q 값을 업데이트해 근사화한다.\n",
        "\n",
        "실제 값과 예측값의 오류 최소화와 L2 규제(Regularization)을 고려한 비용 함수식은 다음과 같다.\n",
        "> min∑(r(u,i) - pu*qi^t)^2 +  λ(||qi||^2 + ||pu||^2)\n",
        "\n",
        "일반적으로 사용자-아이템 평점 행렬의 겨웅 행렬 분해를 위해서 단순히 예측 오류값의 최소화와 학습시 과적합을 피하기 위해서 규제를 반영한 비용 함수를 적용한다. 그리고 위의 비용 함수를 최소화하기 위해서 새롭게 업데이트되는 'pu와 'qi는 다음과 같이 계산할 수 있다(식의 유도는 이 책의 범위를 벗어나 생략한다).\n",
        "  > 'pu = pu + η(e(u,i) * qi - λ * pu)\n",
        "  > 'qi = qi + η(e(u,i) * pu - λ * qi)\n",
        "비용 함수식과 업데이트식의 기호가 의미하는 바는 다음과 같다.\n",
        " - pu: P 행렬의 사용자 u행 벡터\n",
        " - qi^t: Q 행렬의 아이템 i행의 전치 벡터(transpose vector)\n",
        " - r(u,i): 실제 R 행의 u행 i열에 위치한 값.\n",
        " - ^r(u,i): 예측 ^R 행렬의 u행, i열에 위치한 값. pu * qi^t로 계산\n",
        " - e(u,i): u행, i열에 위치한 실제 행렬 값과 예측 행렬 값의 차이 오류. r(u,i) - ^r(u,i)\n",
        " - η: SGD 학습률\n",
        " - λ: L2 규제(Regularization) 계수\n",
        "\n",
        "5장 3절에서 설명한 경사 하강법을 이용한 회귀는 비용 함수를 최소화하는 방향성을 가지고 회귀 계수의 업데이트 값(w1_update, w0_update)을 구하고 이 업데이트 값을 회귀 계수에 반복적으로 적용하는 것이 핵심 로직이었다. 평점 행렬을 경사 하강법을 이용해 행렬 분해하는 것도 이와 유사하다. L2 규제를 반영해 실제 R 행렬 값과 예측 R 행렬 값의 차이를 최소화하는 방향성을 가지고 P행렬과 Q행렬에 업데이트 값을 반복적으로 수행하면서 최적화된 예측 R 행렬을 구하는 방식이 SGD 기반의 행렬 분해이다.\n",
        "\n",
        "이제 SGD를 이용해 행렬 분해를 수행하는 예제를 파이썬으로 구현한다. 분해하려는 원본 행렬 R을 P와 Q로 분해한 뒤에 다시 P와 Q.T의 내적으로 예측 행렬을 만드는 예제이다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XFA2xEkZS-VX"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "# 원본 행렬 R 생성, 분해 행렬 P와 Q 초기화, 잠재 요인 차원 K는 3으로 설정.\n",
        "R = np.array([[4, np.NaN, np.NaN, 2, np.NaN],\n",
        "              [np.NaN, 5, np.NaN, 3, 1],\n",
        "              [np.NaN, np.NaN, 3, 4, 4],\n",
        "              [5, 2, 1, 2, np.NaN]])\n",
        "\n",
        "num_users, num_items = R.shape\n",
        "K=3\n",
        "\n",
        "# P와 Q 행렬의 크기를 지정하고 정규 분포를 가진 임의의 값으로 입력한다.\n",
        "np.random.seed(1)\n",
        "P = np.random.normal(scale=1./K, size=(num_users, K))\n",
        "Q = np.random.normal(scale=1./K, size=(num_items, K))"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4EcPMZr2TsxF"
      },
      "source": [
        "다음으로 실제 R 행렬과 예측 행렬의 오차를 구하는 get_rmse( ) 함수를 만든다. get_rmse( ) 함수는 실제 R 행렬의 널이 아닌 행렬 값의 위치 인덱스를 추출해 이 인덱스에 있는 실제 R 행렬 값과 분해된 P, Q를 이용해 다시 조합된 예측 행렬 값의 RMSE 값을 반환한다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rWew-qw4T54X"
      },
      "source": [
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "def get_rmse(R, P, Q, non_zeros):\n",
        "  error = 0\n",
        "  # 두 개의 분해된 행렬 P와 Q.T의 내적으로 예측 R 행렬 생성\n",
        "  full_pred_matrix=np.dot(P, Q.T)\n",
        "\n",
        "  # 실제 R 행렬에서 널이 아닌 값의 위치 인덱스 추출해 실제 R 행렬과 예측 행렬의 RMSE 추출\n",
        "  x_non_zero_ind = [non_zero[0] for non_zero in non_zeros]\n",
        "  y_non_zero_ind = [non_zero[1] for non_zero in non_zeros]\n",
        "  R_non_zeros = R[x_non_zero_ind, y_non_zero_ind]\n",
        "  full_pred_matrix_non_zeros = full_pred_matrix[x_non_zero_ind, y_non_zero_ind]\n",
        "  mse = mean_squared_error(R_non_zeros, full_pred_matrix_non_zeros)\n",
        "  rmse = np.sqrt(mse)\n",
        "\n",
        "  return rmse"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uk4Rr1YmWIx2"
      },
      "source": [
        "이제 SGD 기반으로 행렬 분해를 수행한다. 먼저 R에서 널 값을 제외한 데이터의 행렬 인덱스를 추출한다. steps는 SGD를 반복해서 업데이트할 횟수를 의미하며, learning_rate는 SGD의 학습률, r_lambda는 L2_Regularization 계수이다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "62O5mJ-FW23m",
        "outputId": "539a86d9-4cf5-4f48-dc03-022353d38adb"
      },
      "source": [
        "# R>0인 행 위치, 열 위치, 값을 non_zeros 리스트에 저장.\n",
        "non_zeros = [ (i , j, R[i,j]) for i in range(num_users) for j in range(num_items) if R[i,j]>0]\n",
        "\n",
        "steps=1000\n",
        "learning_rate=0.01\n",
        "r_lambda=0.01\n",
        "\n",
        "# SGD 기법으로 P와 Q 매트릭스를 계속 업데이트.\n",
        "for step in range(steps):\n",
        "  for i, j, r in non_zeros:\n",
        "    # 실제 값과 예측 값의 차이인 오류 값 구함\n",
        "    eij = r - np.dot(P[i,:],Q[j,:].T)\n",
        "    # Regularization을 반영한 SGD 업데이트 공식 적용\n",
        "    P[i, :] = P[i, :] + learning_rate * (eij * Q[j, :] - r_lambda * P[i, :])\n",
        "    Q[j, :] = Q[j, :] + learning_rate * (eij * P[i, :] - r_lambda * Q[j, :])\n",
        "\n",
        "    rmse = get_rmse(R, P, Q, non_zeros)\n",
        "    if (step % 50) == 0:\n",
        "      print('### iteration step : ', step, 'rmse : ', rmse)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "### iteration step :  0 rmse :  3.261355059488935\n",
            "### iteration step :  0 rmse :  3.26040057174686\n",
            "### iteration step :  0 rmse :  3.253984404542389\n",
            "### iteration step :  0 rmse :  3.2521583839863624\n",
            "### iteration step :  0 rmse :  3.252335303789125\n",
            "### iteration step :  0 rmse :  3.251072196430487\n",
            "### iteration step :  0 rmse :  3.2492449982564864\n",
            "### iteration step :  0 rmse :  3.247416477570409\n",
            "### iteration step :  0 rmse :  3.241926055455223\n",
            "### iteration step :  0 rmse :  3.2400454107613084\n",
            "### iteration step :  0 rmse :  3.240166740749792\n",
            "### iteration step :  0 rmse :  3.2388050277987723\n",
            "### iteration step :  50 rmse :  0.5003190892212749\n",
            "### iteration step :  50 rmse :  0.5001616291326989\n",
            "### iteration step :  50 rmse :  0.4989960120257809\n",
            "### iteration step :  50 rmse :  0.498848345014583\n",
            "### iteration step :  50 rmse :  0.4989518925663175\n",
            "### iteration step :  50 rmse :  0.4983323683009099\n",
            "### iteration step :  50 rmse :  0.4984148489378701\n",
            "### iteration step :  50 rmse :  0.49792599580240876\n",
            "### iteration step :  50 rmse :  0.4900605568692785\n",
            "### iteration step :  50 rmse :  0.4890370238665434\n",
            "### iteration step :  50 rmse :  0.48869176023997846\n",
            "### iteration step :  50 rmse :  0.4876723101369647\n",
            "### iteration step :  100 rmse :  0.15911521988578556\n",
            "### iteration step :  100 rmse :  0.15880916178010931\n",
            "### iteration step :  100 rmse :  0.15874092217089011\n",
            "### iteration step :  100 rmse :  0.15828569528425077\n",
            "### iteration step :  100 rmse :  0.15830809482168753\n",
            "### iteration step :  100 rmse :  0.15828832993767397\n",
            "### iteration step :  100 rmse :  0.15787486893092847\n",
            "### iteration step :  100 rmse :  0.15792073606567075\n",
            "### iteration step :  100 rmse :  0.15725245215457076\n",
            "### iteration step :  100 rmse :  0.15710664164665206\n",
            "### iteration step :  100 rmse :  0.1569025214419\n",
            "### iteration step :  100 rmse :  0.15643403848192458\n",
            "### iteration step :  150 rmse :  0.07546004875264456\n",
            "### iteration step :  150 rmse :  0.07544589133447119\n",
            "### iteration step :  150 rmse :  0.07543234329653034\n",
            "### iteration step :  150 rmse :  0.07514800672233916\n",
            "### iteration step :  150 rmse :  0.07518867696418184\n",
            "### iteration step :  150 rmse :  0.07522889509938421\n",
            "### iteration step :  150 rmse :  0.0748931886446927\n",
            "### iteration step :  150 rmse :  0.07493400425933266\n",
            "### iteration step :  150 rmse :  0.07462695506527875\n",
            "### iteration step :  150 rmse :  0.07464332131959682\n",
            "### iteration step :  150 rmse :  0.07464441641563421\n",
            "### iteration step :  150 rmse :  0.07455141311978064\n",
            "### iteration step :  200 rmse :  0.0436101657943908\n",
            "### iteration step :  200 rmse :  0.043709130689530334\n",
            "### iteration step :  200 rmse :  0.04369072102767999\n",
            "### iteration step :  200 rmse :  0.04347554983227157\n",
            "### iteration step :  200 rmse :  0.04353130925373599\n",
            "### iteration step :  200 rmse :  0.04359240037575308\n",
            "### iteration step :  200 rmse :  0.04329647906053857\n",
            "### iteration step :  200 rmse :  0.04332057192123641\n",
            "### iteration step :  200 rmse :  0.043104482945025314\n",
            "### iteration step :  200 rmse :  0.043135502866585615\n",
            "### iteration step :  200 rmse :  0.04313786864806276\n",
            "### iteration step :  200 rmse :  0.043252267985793146\n",
            "### iteration step :  250 rmse :  0.0293951831856098\n",
            "### iteration step :  250 rmse :  0.029544029484371757\n",
            "### iteration step :  250 rmse :  0.029501874367581973\n",
            "### iteration step :  250 rmse :  0.029329609713572624\n",
            "### iteration step :  250 rmse :  0.029402118073276706\n",
            "### iteration step :  250 rmse :  0.029467205684175147\n",
            "### iteration step :  250 rmse :  0.029189294191791382\n",
            "### iteration step :  250 rmse :  0.029198757426747685\n",
            "### iteration step :  250 rmse :  0.02899574226000236\n",
            "### iteration step :  250 rmse :  0.029044154450545513\n",
            "### iteration step :  250 rmse :  0.029049587101179625\n",
            "### iteration step :  250 rmse :  0.029248328780879226\n",
            "### iteration step :  300 rmse :  0.02267871523374936\n",
            "### iteration step :  300 rmse :  0.022844873864300484\n",
            "### iteration step :  300 rmse :  0.02277356665032507\n",
            "### iteration step :  300 rmse :  0.022632345073225105\n",
            "### iteration step :  300 rmse :  0.022720062551531153\n",
            "### iteration step :  300 rmse :  0.022778917442558343\n",
            "### iteration step :  300 rmse :  0.02251624306238127\n",
            "### iteration step :  300 rmse :  0.022515508246519742\n",
            "### iteration step :  300 rmse :  0.02229491665298547\n",
            "### iteration step :  300 rmse :  0.022367287171783122\n",
            "### iteration step :  300 rmse :  0.02239230348065316\n",
            "### iteration step :  300 rmse :  0.022621116143829507\n",
            "### iteration step :  350 rmse :  0.019516973680183958\n",
            "### iteration step :  350 rmse :  0.019681605297160662\n",
            "### iteration step :  350 rmse :  0.019585635379668606\n",
            "### iteration step :  350 rmse :  0.019467165455250065\n",
            "### iteration step :  350 rmse :  0.019565686789792668\n",
            "### iteration step :  350 rmse :  0.019614020075870636\n",
            "### iteration step :  350 rmse :  0.019368393329296515\n",
            "### iteration step :  350 rmse :  0.019361014872335155\n",
            "### iteration step :  350 rmse :  0.019116038405167703\n",
            "### iteration step :  350 rmse :  0.019209815479975148\n",
            "### iteration step :  350 rmse :  0.019255623979392383\n",
            "### iteration step :  350 rmse :  0.019493636196525232\n",
            "### iteration step :  400 rmse :  0.01803666559195474\n",
            "### iteration step :  400 rmse :  0.018191331063344373\n",
            "### iteration step :  400 rmse :  0.018078504374883713\n",
            "### iteration step :  400 rmse :  0.01797554592952722\n",
            "### iteration step :  400 rmse :  0.018080509676855996\n",
            "### iteration step :  400 rmse :  0.018118882879536797\n",
            "### iteration step :  400 rmse :  0.017889686482489495\n",
            "### iteration step :  400 rmse :  0.017878066671070485\n",
            "### iteration step :  400 rmse :  0.017612244339685706\n",
            "### iteration step :  400 rmse :  0.01772096734904676\n",
            "### iteration step :  400 rmse :  0.017781796456597727\n",
            "### iteration step :  400 rmse :  0.018022719092132773\n",
            "### iteration step :  450 rmse :  0.017334045429542207\n",
            "### iteration step :  450 rmse :  0.017476834937591626\n",
            "### iteration step :  450 rmse :  0.017353619075108358\n",
            "### iteration step :  450 rmse :  0.017260553985290757\n",
            "### iteration step :  450 rmse :  0.017369093850106612\n",
            "### iteration step :  450 rmse :  0.017399933857257795\n",
            "### iteration step :  450 rmse :  0.0171843175786375\n",
            "### iteration step :  450 rmse :  0.0171699064962513\n",
            "### iteration step :  450 rmse :  0.016888615795792955\n",
            "### iteration step :  450 rmse :  0.01700663815408313\n",
            "### iteration step :  450 rmse :  0.017076792508661655\n",
            "### iteration step :  450 rmse :  0.01731968595344283\n",
            "### iteration step :  500 rmse :  0.01699160924805319\n",
            "### iteration step :  500 rmse :  0.0171234089157865\n",
            "### iteration step :  500 rmse :  0.01699398405641072\n",
            "### iteration step :  500 rmse :  0.016907070492030378\n",
            "### iteration step :  500 rmse :  0.017017605772217764\n",
            "### iteration step :  500 rmse :  0.01704327755670073\n",
            "### iteration step :  500 rmse :  0.016838031459003783\n",
            "### iteration step :  500 rmse :  0.016821674312725587\n",
            "### iteration step :  500 rmse :  0.01652928126442942\n",
            "### iteration step :  500 rmse :  0.016652888795198606\n",
            "### iteration step :  500 rmse :  0.016728541275491084\n",
            "### iteration step :  500 rmse :  0.016973657887570985\n",
            "### iteration step :  550 rmse :  0.016818969716266428\n",
            "### iteration step :  550 rmse :  0.01694144559744487\n",
            "### iteration step :  550 rmse :  0.016808259298884326\n",
            "### iteration step :  550 rmse :  0.016725234339747642\n",
            "### iteration step :  550 rmse :  0.016836938491435156\n",
            "### iteration step :  550 rmse :  0.016859187050621244\n",
            "### iteration step :  550 rmse :  0.016661644526141703\n",
            "### iteration step :  550 rmse :  0.016643851020065135\n",
            "### iteration step :  550 rmse :  0.016343446075494296\n",
            "### iteration step :  550 rmse :  0.016470440821826447\n",
            "### iteration step :  550 rmse :  0.016549323314269518\n",
            "### iteration step :  550 rmse :  0.01679680459589558\n",
            "### iteration step :  600 rmse :  0.016727439717439153\n",
            "### iteration step :  600 rmse :  0.016842259158977278\n",
            "### iteration step :  600 rmse :  0.01670668792446753\n",
            "### iteration step :  600 rmse :  0.016626255644609494\n",
            "### iteration step :  600 rmse :  0.016738696939262863\n",
            "### iteration step :  600 rmse :  0.01675868241598574\n",
            "### iteration step :  600 rmse :  0.01656685720005292\n",
            "### iteration step :  600 rmse :  0.016547954461110775\n",
            "### iteration step :  600 rmse :  0.01624166876076104\n",
            "### iteration step :  600 rmse :  0.01637080005613797\n",
            "### iteration step :  600 rmse :  0.0164516272092571\n",
            "### iteration step :  600 rmse :  0.016701322901884634\n",
            "### iteration step :  650 rmse :  0.016674291334806118\n",
            "### iteration step :  650 rmse :  0.016782895588884888\n",
            "### iteration step :  650 rmse :  0.01664569809164759\n",
            "### iteration step :  650 rmse :  0.01656714079916211\n",
            "### iteration step :  650 rmse :  0.016680091021598505\n",
            "### iteration step :  650 rmse :  0.016698554271430695\n",
            "### iteration step :  650 rmse :  0.01651101773242772\n",
            "### iteration step :  650 rmse :  0.016491228766905206\n",
            "### iteration step :  650 rmse :  0.016180544197961624\n",
            "### iteration step :  650 rmse :  0.01631111150707523\n",
            "### iteration step :  650 rmse :  0.01639316772050073\n",
            "### iteration step :  650 rmse :  0.016644736912476574\n",
            "### iteration step :  700 rmse :  0.016638362442608513\n",
            "### iteration step :  700 rmse :  0.0167419367433236\n",
            "### iteration step :  700 rmse :  0.016603524189001635\n",
            "### iteration step :  700 rmse :  0.016526454393300506\n",
            "### iteration step :  700 rmse :  0.016639792083379536\n",
            "### iteration step :  700 rmse :  0.016657201345297357\n",
            "### iteration step :  700 rmse :  0.01647292838164141\n",
            "### iteration step :  700 rmse :  0.01645241257047359\n",
            "### iteration step :  700 rmse :  0.016138379086448086\n",
            "### iteration step :  700 rmse :  0.016269993747904974\n",
            "### iteration step :  700 rmse :  0.01635288508504548\n",
            "### iteration step :  700 rmse :  0.016605910068210012\n",
            "### iteration step :  750 rmse :  0.01660906046895513\n",
            "### iteration step :  750 rmse :  0.01670856296909822\n",
            "### iteration step :  750 rmse :  0.016569153528341703\n",
            "### iteration step :  750 rmse :  0.016493367054249884\n",
            "### iteration step :  750 rmse :  0.01660702796687089\n",
            "### iteration step :  750 rmse :  0.016623681027525393\n",
            "### iteration step :  750 rmse :  0.01644192727172457\n",
            "### iteration step :  750 rmse :  0.016420802465343706\n",
            "### iteration step :  750 rmse :  0.016104179990850825\n",
            "### iteration step :  750 rmse :  0.016236628551952983\n",
            "### iteration step :  750 rmse :  0.01632014100929194\n",
            "### iteration step :  750 rmse :  0.01657420047570488\n",
            "### iteration step :  800 rmse :  0.016581161561119988\n",
            "### iteration step :  800 rmse :  0.016677363428437137\n",
            "### iteration step :  800 rmse :  0.016537069269613805\n",
            "### iteration step :  800 rmse :  0.01646246137777885\n",
            "### iteration step :  800 rmse :  0.016576412350487686\n",
            "### iteration step :  800 rmse :  0.016592501800249713\n",
            "### iteration step :  800 rmse :  0.016412717409428523\n",
            "### iteration step :  800 rmse :  0.016391072859801684\n",
            "### iteration step :  800 rmse :  0.016072423077368784\n",
            "### iteration step :  800 rmse :  0.016205589842522038\n",
            "### iteration step :  800 rmse :  0.016289609430091508\n",
            "### iteration step :  800 rmse :  0.01654431582921612\n",
            "### iteration step :  850 rmse :  0.016552228984315465\n",
            "### iteration step :  850 rmse :  0.016645751215475714\n",
            "### iteration step :  850 rmse :  0.016504627328190483\n",
            "### iteration step :  850 rmse :  0.01643114580174884\n",
            "### iteration step :  850 rmse :  0.016545370571042384\n",
            "### iteration step :  850 rmse :  0.016561024020104998\n",
            "### iteration step :  850 rmse :  0.016382795627019667\n",
            "### iteration step :  850 rmse :  0.01636070007608577\n",
            "### iteration step :  850 rmse :  0.016040446344395495\n",
            "### iteration step :  850 rmse :  0.016174269580681248\n",
            "### iteration step :  850 rmse :  0.01625873735464131\n",
            "### iteration step :  850 rmse :  0.016513751774735037\n",
            "### iteration step :  900 rmse :  0.01652128043377805\n",
            "### iteration step :  900 rmse :  0.016612624200841565\n",
            "### iteration step :  900 rmse :  0.016470695682262133\n",
            "### iteration step :  900 rmse :  0.016398314989165507\n",
            "### iteration step :  900 rmse :  0.01651280633307365\n",
            "### iteration step :  900 rmse :  0.016528110873501898\n",
            "### iteration step :  900 rmse :  0.016351122754892623\n",
            "### iteration step :  900 rmse :  0.01632862978384231\n",
            "### iteration step :  900 rmse :  0.016007096878603345\n",
            "### iteration step :  900 rmse :  0.0161415440715142\n",
            "### iteration step :  900 rmse :  0.01622643084399404\n",
            "### iteration step :  900 rmse :  0.01648146573819507\n",
            "### iteration step :  950 rmse :  0.016488081335748327\n",
            "### iteration step :  950 rmse :  0.016577652134974738\n",
            "### iteration step :  950 rmse :  0.016434929334981776\n",
            "### iteration step :  950 rmse :  0.016363636620406127\n",
            "### iteration step :  950 rmse :  0.016478391959548654\n",
            "### iteration step :  950 rmse :  0.01649340903060661\n",
            "### iteration step :  950 rmse :  0.016317416842511017\n",
            "### iteration step :  950 rmse :  0.01629456857175322\n",
            "### iteration step :  950 rmse :  0.015972009545965202\n",
            "### iteration step :  950 rmse :  0.01610706345879575\n",
            "### iteration step :  950 rmse :  0.016192355609214667\n",
            "### iteration step :  950 rmse :  0.01644717168347911\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oLxld80MX7F2"
      },
      "source": [
        "이제 분해된 P와 Q 함수를 P*Q.T로 예측 행렬을 만들어 출력해 본다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PfOudV9pX_CP",
        "outputId": "6002423e-c2c7-4cce-9b34-e96454124b24"
      },
      "source": [
        "pred_matrix = np.dot(P, Q.T)\n",
        "print('예측 행렬:\\n', np.round(pred_matrix, 3))"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "예측 행렬:\n",
            " [[3.991 0.897 1.306 2.002 1.663]\n",
            " [6.696 4.978 0.979 2.981 1.003]\n",
            " [6.677 0.391 2.987 3.977 3.986]\n",
            " [4.968 2.005 1.006 2.017 1.14 ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IzeeZsolYawx"
      },
      "source": [
        "원본 행렬과 비교해 널이 아닌 값은 큰 차이가 나지 않으며, 널인 값은 새로운 예측값으로 채워졌다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lB38iZTCbFoi"
      },
      "source": [
        "# 5. 콘텐츠 기반 필터링 실습 - TMDB 5000 영화 데이터 세트\n",
        "\n",
        "해당 데이터는 https://www.kaggle.com/tmdb/tmdb-movie-metadata 에서 내려받을 수 있다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J_oQ8KWyb6tZ"
      },
      "source": [
        "## 1) 장르 속성을 이용한 영화 콘텐츠 기반 필터링\n",
        "\n",
        "**콘텐츠 기반 필터링**은 사용자가 특정 영화를 감상하고 그 영화를 좋아했다면 그 영화와 비슷한 특성/속성, 구성 요소 등을 가진 다른 영화를 추천하는 것이다. 가령 영화 '인셉션'을 재미있게 봤다면 '인셉션'의 장르인 액션, 공상과학으로 높은 평점을 받은 다른 영화를 추천하거나 '인셉션'의 감독인 크리스토퍼 놀란의 다른 영화를 추천하는 방식이다. 이렇게 영화(또는 상품/서비스) 간의 유사성을 판단하는 기준이 영화를 구성하는 다양한 콘텐츠(장르, 감독, 배우, 평점, 키워드, 영화 설명)를 기반으로 하는 방식이 바로 콘텐츠 기반 필터링이다.\n",
        "\n",
        "콘텐츠 기반 필터링 추천 시스템을 영화를 선택하는 데 중요한 요소인 영화 장르 속성을 기반으로 만들어 본다. 장르 칼럼 값의 유사도를 비교한 뒤 그중 높은 평점을 가지는 영화를 추천하는 방식이다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5IKUQOr8chMy"
      },
      "source": [
        "## 2) 데이터 로딩 및 가공\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 202
        },
        "id": "fOe9-jFTcqpW",
        "outputId": "5ff87013-9db8-4ce0-9675-0e5c45412137"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import warnings; warnings.filterwarnings('ignore')\n",
        "\n",
        "movies = pd.read_csv('/content/drive/MyDrive/데이터분석/파이썬머신러닝완벽가이드/tmdb_5000/tmdb_5000_movies.csv')\n",
        "print(movies.shape)\n",
        "movies.head(1)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(4803, 20)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>budget</th>\n",
              "      <th>genres</th>\n",
              "      <th>homepage</th>\n",
              "      <th>id</th>\n",
              "      <th>keywords</th>\n",
              "      <th>original_language</th>\n",
              "      <th>original_title</th>\n",
              "      <th>overview</th>\n",
              "      <th>popularity</th>\n",
              "      <th>production_companies</th>\n",
              "      <th>production_countries</th>\n",
              "      <th>release_date</th>\n",
              "      <th>revenue</th>\n",
              "      <th>runtime</th>\n",
              "      <th>spoken_languages</th>\n",
              "      <th>status</th>\n",
              "      <th>tagline</th>\n",
              "      <th>title</th>\n",
              "      <th>vote_average</th>\n",
              "      <th>vote_count</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>237000000</td>\n",
              "      <td>[{\"id\": 28, \"name\": \"Action\"}, {\"id\": 12, \"nam...</td>\n",
              "      <td>http://www.avatarmovie.com/</td>\n",
              "      <td>19995</td>\n",
              "      <td>[{\"id\": 1463, \"name\": \"culture clash\"}, {\"id\":...</td>\n",
              "      <td>en</td>\n",
              "      <td>Avatar</td>\n",
              "      <td>In the 22nd century, a paraplegic Marine is di...</td>\n",
              "      <td>150.437577</td>\n",
              "      <td>[{\"name\": \"Ingenious Film Partners\", \"id\": 289...</td>\n",
              "      <td>[{\"iso_3166_1\": \"US\", \"name\": \"United States o...</td>\n",
              "      <td>2009-12-10</td>\n",
              "      <td>2787965087</td>\n",
              "      <td>162.0</td>\n",
              "      <td>[{\"iso_639_1\": \"en\", \"name\": \"English\"}, {\"iso...</td>\n",
              "      <td>Released</td>\n",
              "      <td>Enter the World of Pandora.</td>\n",
              "      <td>Avatar</td>\n",
              "      <td>7.2</td>\n",
              "      <td>11800</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "      budget  ... vote_count\n",
              "0  237000000  ...      11800\n",
              "\n",
              "[1 rows x 20 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WPvq2SNxdbBu"
      },
      "source": [
        "여러 개의 피처 중 콘텐츠 기반 필터링 추천 분석에 사용할 주요 칼럼만 추출해 새롭게 DataFrame으로 만든다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jxyfb3aPdhCx"
      },
      "source": [
        "movies_df = movies[['id', 'title', 'genres', 'vote_average', 'vote_count', 'popularity', 'keywords', 'overview']]"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LsoIRXhSd-bt"
      },
      "source": [
        "데이터를 보면 주의해야 할 칼럼이 있다. 'genres', 'keywords' 등과 같은 칼럼을 보면 [{'id': 28, 'name': 'Action'}, {'id': 12, 'name': 'Adventure'}]와 같이 파이썬 리스트(list) 내부에 여러 개의 딕셔너리(dict)가 있는 형태의 문자열로 표기돼 있다. 이는 한꺼번에 여러 개의 값을 표현하기 위한 표기 방식이다. 하지만 이 칼럼이 DataFrame으로 만들어질 때는 단순히 문자열 형태로 로딩되므로 이 칼럼을 가공하지 않고는 필요한 정보를 추출할 수가 없다. 먼저 해당 칼럼이 어떤 형태로 돼 있는지 확인한다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 97
        },
        "id": "qYHI6nfXefDH",
        "outputId": "b411484f-3119-42cf-f2b1-05ad1abe7a15"
      },
      "source": [
        "pd.set_option('max_colwidth', 100)\n",
        "movies_df[['genres', 'keywords']][:1]"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>genres</th>\n",
              "      <th>keywords</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>[{\"id\": 28, \"name\": \"Action\"}, {\"id\": 12, \"name\": \"Adventure\"}, {\"id\": 14, \"name\": \"Fantasy\"}, {...</td>\n",
              "      <td>[{\"id\": 1463, \"name\": \"culture clash\"}, {\"id\": 2964, \"name\": \"future\"}, {\"id\": 3386, \"name\": \"sp...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                                                                genres                                                                                             keywords\n",
              "0  [{\"id\": 28, \"name\": \"Action\"}, {\"id\": 12, \"name\": \"Adventure\"}, {\"id\": 14, \"name\": \"Fantasy\"}, {...  [{\"id\": 1463, \"name\": \"culture clash\"}, {\"id\": 2964, \"name\": \"future\"}, {\"id\": 3386, \"name\": \"sp..."
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HcRACvhaeoZQ"
      },
      "source": [
        "위와 같이 genres 칼럼은 여러 개의 개별 장르 데이터를 가지고 있고, 이 개별 장르의 명칭은 딕셔너리 키(Key)인 'name'으로 추출할 수 있다. Keywords 역시 마찬가지 구조를 가지고 있다. genres 칼럼의 문자열을 분해해서 개별 장르를 파이썬 리스트 객체로 추출한다 파이썬 ast 모듈의 **literal_eval( )** 함수를 이용하면 이 문자열을 문자열이 의미하는 list [dict1, dict2] 객체로 만들 수 있다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jCcm7sThe6y5"
      },
      "source": [
        "from ast import literal_eval\n",
        "movies_df['genres'] = movies_df['genres'].apply(literal_eval)\n",
        "movies_df['keywords'] = movies_df['keywords'].apply(literal_eval)"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WopGz67lffTA"
      },
      "source": [
        "이제 genres 칼럼은 문자열이 아니라 실제 리스트 내부에 여러 장르 딕셔너리로 구성된 객체를 가진다. 이제 장르명만 리스트 객체로 추출하도록 한다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 80
        },
        "id": "Aqgtlej6f1cz",
        "outputId": "efe485a0-9b0b-4118-dd5a-3932907b9e74"
      },
      "source": [
        "movies_df['genres'] = movies_df['genres'].apply(lambda x : [y['name'] for y in x])\n",
        "movies_df['keywords'] = movies_df['keywords'].apply(lambda x : [y['name'] for y in x])\n",
        "movies_df[['genres', 'keywords']][:1]"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>genres</th>\n",
              "      <th>keywords</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>[Action, Adventure, Fantasy, Science Fiction]</td>\n",
              "      <td>[culture clash, future, space war, space colony, society, space travel, futuristic, romance, spa...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                          genres                                                                                             keywords\n",
              "0  [Action, Adventure, Fantasy, Science Fiction]  [culture clash, future, space war, space colony, society, space travel, futuristic, romance, spa..."
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n_1gUzzvkaz-"
      },
      "source": [
        "## 3) 장르 콘텐츠 유사도 측정\n",
        "\n",
        "장르별 유사도를 어떻게 측정할까? 여러 가지 방법이 있을 수 있으나, 가장 간단한 방법은 genres를 문자열로 변경한 뒤 이를 CountVectorizer로 피처 벡터화한 행렬 데이터 값을 코사인 유사도로 비교하는 것이다. genres 칼럼을 기반으로 하는 콘텐츠 기반 필터링은 다음과 같은 단계로 구현한다.\n",
        " - 문자열로 변환된 genres 칼럼을 Count 기반으로 피처 벡터화 변환한다.\n",
        " - genres 문자열을 피처 벡터화 행렬로 변환한 데이터 세트를 코사인 유사도를 통해 비교한다. 이를 위해 데이터 세트의 레코드별로 타 레코드와 장르에서 코사인 유사도 값을 가지는 객체를 생성한다.\n",
        " - 장르 유사도가 높은 영화 중에 평점이 높은 순으로 영화를 추천한다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mdtdcQQBl7VF"
      },
      "source": [
        "먼저 genres 칼럼을 문자열로 변환한 뒤 사이킷런의 CountVectorizer를 이용해 피처 벡터 행렬로 만든다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E95W6HcPnpLE",
        "outputId": "48ccabc4-2173-46fc-87c6-618ba6f81669"
      },
      "source": [
        "movies_df['genres']"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0       [Action, Adventure, Fantasy, Science Fiction]\n",
              "1                        [Adventure, Fantasy, Action]\n",
              "2                          [Action, Adventure, Crime]\n",
              "3                    [Action, Crime, Drama, Thriller]\n",
              "4                [Action, Adventure, Science Fiction]\n",
              "                            ...                      \n",
              "4798                        [Action, Crime, Thriller]\n",
              "4799                                [Comedy, Romance]\n",
              "4800               [Comedy, Drama, Romance, TV Movie]\n",
              "4801                                               []\n",
              "4802                                    [Documentary]\n",
              "Name: genres, Length: 4803, dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6G-VQGPdnFsn",
        "outputId": "5ed842b7-7777-484b-894c-386e0c8c3066"
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# CountVectorizer를 적용하기 위해 공백문자로 word 단위가 구분되는 문자열로 변환\n",
        "movies_df['genres_literal'] = movies_df['genres'].apply(lambda x: (' ').join(x))\n",
        "count_vect = CountVectorizer(min_df=0, ngram_range=(1, 2))\n",
        "genre_mat = count_vect.fit_transform(movies_df['genres_literal'])\n",
        "print(genre_mat.shape)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(4803, 276)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zgzn2gJ_n9Vt"
      },
      "source": [
        "CountVectorizer로 변환해 4803개의 레코드와 276개의 개별 단어 피처로 구성된 피처 벡터 행렬이 만들어졌다. 이렇게 생성된 피처 벡터 행렬에 사이킷런의 cosine_similarity( )를 이용해 코사인 유사도를 계산한다. 반환된 코사인 유사도 행렬의 크기 및 앞 2개 데이터만 추출해본다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KExmh6dDoPmS",
        "outputId": "84b7fe13-be48-4d5e-ee9b-d2258520fbe4"
      },
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "genre_sim = cosine_similarity(genre_mat, genre_mat)\n",
        "print(genre_sim.shape)\n",
        "print(genre_sim[:2])"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(4803, 4803)\n",
            "[[1.         0.59628479 0.4472136  ... 0.         0.         0.        ]\n",
            " [0.59628479 1.         0.4        ... 0.         0.         0.        ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6NwMvrW5opaN"
      },
      "source": [
        "cosine_similarities( ) 호출로 생성된 genre_sim 객체는 movies_df의 genre_literal 칼럼을 피처 벡터화한 행렬(genre_mat) 데이터의 행(레코드)별 유사도 정보를 가지고 있으며, 결국은 movies_df DataFrame의 행별 장르 유사도 값을 가지고 있는 것이다. movies_df를 장르 기준으로 콘텐츠 기반 필터링을 수행하려면 movies_df의 개별 레코드에 대해서 가장 장르 유사도가 높은 순으로 다른 레코드를 추출해야 하는데, 이를 위해 앞에서 생성한 genre_sim 객체를 이용한다.\n",
        "\n",
        "genre_sim 객체의 기준 행별로 비교 대상이 되는 행의 유사도 값이 높은 순으로 정렬된 행렬의 위치 인덱스 값을 추출하면 된다. 값이 높은 순으로 정렬된 비교 대상 행의 유사도 값이 아니라 비교 대상 행의 위치 인덱스임에 주의해야 한다. 넘파이의 argsort( ) 함수를 이용한다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tabYjeuFpPV8",
        "outputId": "61bb49ae-140a-42a4-b395-ff483c3c573b"
      },
      "source": [
        "genre_sim_sorted_ind = genre_sim.argsort()[:, ::-1]\n",
        "print(genre_sim_sorted_ind[:1])"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[   0 3494  813 ... 3038 3037 2401]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "op4w1UuVpXxE"
      },
      "source": [
        "반환된 [[   0 3494  813 ... 3038 3037 2401]] 이 의미하는 것은 0번 레코드의 경우 자신인 0번 레코드를 제외하면 3494번 레코드가 가장 유사도가 높고, 그 다음 813번 레코드이며, 가장 유사도가 낮은 레코드는 2401번 레코드라는 뜻이다.\n",
        "\n",
        "이제 genre_sim_sorted_ind 객체는 각 레코드의 장르 코사인 유사도가 가장 높은 순으로 정렬된 타 레코드의 위치 인덱스 값을 가지고 있다. 이 위치 인덱스를 이용해 언제든지 특정 레코드와 코사인 유사도가 높은 다른 레코드를 추출할 수 있다."
      ]
    }
  ]
}