{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "[파이썬머신러닝완벽가이드] IV. 분류.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNvsU0Ji3ZqH5hiwITM2WER",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/spacemaninSeoul/PythonMachinelearningGuide/blob/main/%5B%ED%8C%8C%EC%9D%B4%EC%8D%AC%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D%EC%99%84%EB%B2%BD%EA%B0%80%EC%9D%B4%EB%93%9C%5D_IV_%EB%B6%84%EB%A5%98.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cZJd_DPHWHEB"
      },
      "source": [
        "# 1. 분류(Classification)의 개요\n",
        "\n",
        "지도학습은 레이블(Label), 즉 명시적인 정답이 있는 데이터가 주어진 상태에서 학습하는 머신러닝 방식이다. 지도학습의 대표적인 유형인 분류(Classification)는 학습 데이터로 주어진 데이터의 피처와 레이블값(결정 값, 클래스 값)을 머신러닝 알고리즘으로 학습해 모델을 생성하고, 이렇게 생성된 모델에 새로운 데이터 값이 주어졌을 때 미지의 레이블 값을 예측하는 것이다.\n",
        "\n",
        "즉, 기존 데이터가 어떤 레이블에 속하는지 패턴을 알고리즘으로 인지한 뒤에 새롭게 관측된 데이터에 대한 레이블을 판별하는 것이다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G78dtQ-EWsvo"
      },
      "source": [
        "# 2. 결정 트리\n",
        "\n",
        "결정 트리(Decision Tree)는 데이터에 있는 규칙을 학습을 통해 자동으로 찾아내 트리(Tree) 기반의 분류 규칙을 만드는 것이다.\n",
        "\n",
        "데이터의 어떤 기준을 바탕으로 규칙을 만들어야 가장 효율적인 분류가 될 것인가가 알고리즘의 성능을 크게 좌우한다.\n",
        "\n",
        "결정 트리의 구조에서(책 185p) 규칙 노드(Decision Node)로 표시된 노드는 규칙 조건이 되는 것이고, 리프 노드(Leaf Node)로 표시된 노드는 결정된 클래스 값이다. 그리고 새로운 규칙 조건마다 서브 트리(Sub Tree)가 생성된다. 데이터 세트에 피처가 있고 이러한 피처가 결합해 규칙 조건을 만들 때마다 규칙 노드가 만들어진다. 하지만 많은 규칙이 있다는 것은 곧 분류를 결정하는 방식이 더욱 복잡해진다는 얘기이고, 이는 곧 과적합으로 이어지기 쉽다. 즉, 트리의 깊이(depth)가 깊어질수록 결정 트리의 예측 성능이 저하될 가능성이 높다.\n",
        "\n",
        "결정 노드는 정보 균일도가 높은 데이터 세트를 먼저 선택할 수 있도록 규칙 조건을 만든다. 즉, 정보 균일도가 데이터 세트로 쪼개질 수 있도록 조건을 찾아 서브 데이터 세트를 만들고, 다시 이 서브 데이터 세트에서 균일도가 높은 자식 데이터 세트 쪼개는 방식을 자식 트리로 내려가면서 반복하는 방식으로 데이터 값을 예측하게 된다.\n",
        "\n",
        "정보의 균일도를 측정하는 대표적인 방법은 엔트로피를 이용한 정보 이득(Information Gain) 지수와 지니 계수가 있다.\n",
        "* **정보 이득**은 엔트로피라는 개념을 기반으로 한다. 엔트로피는  주어진 데이터 집합의 혼잡도를 의미한다. 서로 다른 값이 섞여 있으면 엔트로피가 높고, 같은 값이 섞여 있으면 엔트로피 값이 낮다. 정보 이득 지수는 1에서 엔트로피 지수를 뺀 값이다. 즉 ' 1 - 엔트로피 지수 '이다. 결정 트리는 이 정보 이득 지수로 분할 기준을 정한다. 즉, 정보 이득이 높은 (엔트로피 지수가 낮은) 속성을 기준으로 분할한다.\n",
        "* **지니 계수**는 머신러닝에서 적용될 때는 지니 계수가 낮을수록 데이터 균일도가 높은 것으로 해석해 지니 계수가 낮은 속성을 기준으로 분할한다.\n",
        "\n",
        "결정 트리 알고리즘을 사이킷런에서 구현한 DecisionTreeClassifier는 기본으로 지니 계수를 이용해 데이터 세트를 분할한다. 결정 트리의 일반적인 알고리즘은 데이터 세트를 분할하는 데 가장 좋은 조건, 즉 정보 이득이 높거나 지니 계수가 낮은 조건을 찾아서 자식 트리 노드에 걸쳐 반복적으로 분할한 뒤, 데이터가 모두 특정 분류에 속하게 되면 분할을 멈추고 분류를 결정한다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t06lAVQFZ6Jx"
      },
      "source": [
        "## 1) 결정 트리 모델의 특징\n",
        "\n",
        "결정 트리의 가장 큰 장점은 정보의 '균일도'라는 룰을 기반으로 하고 있어서 알고리즘이 쉽고 직관적이라는 점이다. 또 정보의 균일도만 신경 쓰면 되므로 특별한 경우를 제외하고는 각 피처의 스케일링과 정규화 같은 전처리 작업이 필요 없다.\n",
        "\n",
        "반면에 가장 큰 단점으로는 과적합으로 정확도가 떨어진다는 점이다.\n",
        "\n",
        "* **결정 트리 장점** : 쉽다, 직관적이다. 피처의 스케일링이나 정규화 등의 사전 가공 영향도가 크지 않음\n",
        "* **결정 트리 단점** : 과적합으로 알고리즘 성능이 떨어진다. 이를 극복하기 위해 트리의 크기를 사전에 제한하는 튜닝 필요."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k0-O8ewFas1J"
      },
      "source": [
        "## 2) 결정 트리 파라미터\n",
        "\n",
        "사이킷런은 결정 트리 알고리즘을 구현한 분류를 위한 클래스인 DecisionTreeClassifier와 회귀를 위한 클래스인 DecisionTreeRegressor를 제공한다. DecisionTreeClassifier와 DecisionTreeRegressor 모두 파라미터는 다음과 같이 동일한 파라미터를 사용한다.\n",
        "* min_samples_split\n",
        "  * 노드를 분할하기 위한 최소한의 샘플 데이터 수로 과적합을 제어하는 데 사용됨.\n",
        "  * 디폴트는 2, 작게 설정할 수록 분할되는 노드가 많아져서 과적합 가능성 증가.\n",
        "  * 과적합을 제어. 1로 설정할 경우 분할되는 노드가 많아져서 과적합 가능성 증가.\n",
        "\n",
        "* min_samples_leaf\n",
        "  * 말단 노드(Leaf)가 되기 위한 최소한의 샘플 데이터 수.\n",
        "  * 과적합 제어 용도. 그러나 비대칭적(imbalanced) 데이터의 경우 특정 클래스의 데이터가 극도로 작을 수 있으므로 이 경우는 작게 설정 필요.\n",
        "\n",
        "* max_features\n",
        "  * 최적의 분할을 위해 고려할 최대 피처 개수. 디폴트는 None으로 데이터 세트의 모든 피처를 사용해 분할 사용.\n",
        "  * int 형으로 지정하면 대상 피처 개수, float 형으로 지정하면 전체 피처 중 대상 피처의 퍼센트임\n",
        "  * 'sqrt'는 전체 피처 중 sqrt(전체 피처 개수), 즉 루트 전체 피처 개수 만큼 선정\n",
        "  * 'auto'로 지정하면 sqrt와 동일\n",
        "  * 'log'는 전체 피처 중 log2(전체 피처 개수) 선정\n",
        "  * 'None'은 전체 피처 선정\n",
        "\n",
        "* max_depth\n",
        "  * 트리의 최대 깊이를 규정\n",
        "  * 디폴트는 None. None으로 설정하면 완벽하게 클래스 결정 값이 될 때까지 깊이를 계속 키우며 분할하거나 노드가 가지는 데이터 개수가 min_samples_split보다 작아질 때까지 계속 깊이를 증가시킴.\n",
        "  * 깊이가 깊어지면 min_samples_split 설정대로 최대 분할하여 과적합할 수 있으므로 적절한 값으로 제어 필요.\n",
        "\n",
        "* max_leaf_nodes\n",
        "  * 말단 노드(Leaf)의 최대 개수\n",
        "\n"
      ]
    }
  ]
}